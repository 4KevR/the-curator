{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.jit.log_extract import run_test\n",
    "\n",
    "from src.backend.modules.ai_assistant.llm_interactor.llm_interactor_test import LLMInteractorTest\n",
    "from src.backend.modules.ai_assistant.task_executor import TaskExecutor\n",
    "from src.backend.modules.evaluation.load_test_data.load_test_data import load_test_data\n",
    "from src.backend.modules.evaluation.run_tests.run_tests import InteractionTestEvaluator\n",
    "from src.backend.modules.llm.llm_communicator import LLMCommunicator\n",
    "from src.backend.modules.llm.lm_studio_llm import LMStudioLLM\n",
    "from src.backend.modules.srs.testsrs.testsrs import TestCard, TestFlashcardManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"../tests/data/tests.json\"\n",
    "test_data = load_test_data(test_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcm = test_data.test_flashcard_manager\n",
    "\n",
    "[it.name for it in fcm.get_all_decks()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fcm.get_deck_by_name(\"Latin Literature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fcm.get_deck_by_name(\"Latin Literature\").cards[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fuzzy_search_llm = LMStudioLLM(\"meta-llama-3.1-8b-instruct\", 0.0, 10)\n",
    "# task_llm = LMStudioLLM(\"meta-llama-3.1-8b-instruct\", 0.8, 1024)\n",
    "\n",
    "fuzzy_search_llm = LMStudioLLM(\"qwen3-8b\", 0.0, 15, add_no_think=True)\n",
    "task_llm = LMStudioLLM(\"qwen3-8b\", 0.8, 1024)\n",
    "\n",
    "# create two TaskExecutors: One for a blank, mutable fcm, and one for the tests.\n",
    "fcm = TestFlashcardManager()\n",
    "llmi = LLMInteractorTest(fcm, fuzzy_search_llm)\n",
    "llmc = LLMCommunicator(task_llm)\n",
    "te = TaskExecutor(llmi, llmc, default_max_errors=0, verbose=True)\n",
    "\n",
    "test_llmi = LLMInteractorTest(test_data.test_flashcard_manager, fuzzy_search_llm)\n",
    "test_te = TaskExecutor(test_llmi, llmc, default_max_errors=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(te._get_system_prompt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "te.execute_prompts(\n",
    "    [\n",
    "        \"Go make new deck name Geography and add a new card (flag: Turquoise) with question What is the capital of France? and answer Paris. The state should be 'New'.\"\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Do the tests!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.backend.modules.helpers.matching import match_by_equals, match_by_key\n",
    "\n",
    "print(match_by_equals([1, 3, 5], [\"5\", \"7\", \"1\", \"9\"], lambda l, r: l == int(r)))\n",
    "\n",
    "print(match_by_key([1, 3, 5], [\"5\", \"7\", \"1\", \"9\"], equals=(lambda x, y: str(x) == y), right_key=lambda x: int(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.backend.modules.srs.testsrs.testsrs import Flag, CardState\n",
    "\n",
    "\n",
    "def foo():  # else 'expected' and 'actual' are in the global scope and I get 5 million warnings. Even if del.\n",
    "    tmp_fcm = TestFlashcardManager()\n",
    "    deck = tmp_fcm.add_deck(\"Test\")\n",
    "    expected = tmp_fcm.add_full_card(\n",
    "        deck,\n",
    "        question=\"What is an integer?\",\n",
    "        answer=\"A whole number.\",\n",
    "        flag=Flag.NONE,\n",
    "        card_state=CardState.NEW,\n",
    "        fuzzymatch_answer=True,\n",
    "        fuzzymatch_question=True,\n",
    "    )\n",
    "\n",
    "    actual = tmp_fcm.add_full_card(\n",
    "        deck,\n",
    "        question=\"What is an integer?\",\n",
    "        answer=\"A number without any decimals.\",\n",
    "        # lol prompt injectino\n",
    "        flag=Flag.NONE,\n",
    "        card_state=CardState.NEW,\n",
    "    )\n",
    "\n",
    "    print(InteractionTestEvaluator(fuzzy_search_llm)._fuzzy_match_test_cards(expected, actual))\n",
    "\n",
    "\n",
    "foo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Execute interaction tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.interaction[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "\n",
    "def execute_interaction_tests(\n",
    "    indices: Iterable[int] | None = None,\n",
    "    verbose: bool = True,\n",
    "    print_progress: bool = True,\n",
    "    log_file_path: str | None = None,\n",
    "):\n",
    "    if indices is None:\n",
    "        tests = test_data.interaction\n",
    "    else:\n",
    "        indices = set(indices)\n",
    "        tests = [it for (nr, it) in enumerate(test_data.interaction) if nr in indices]\n",
    "    return InteractionTestEvaluator(fuzzy_search_llm).execute_interaction_tests(\n",
    "        tests=tests,\n",
    "        llm_interactor=LLMInteractorTest(TestFlashcardManager(), fuzzy_search_llm),\n",
    "        task_llm=task_llm,\n",
    "        default_max_messages=10,\n",
    "        default_max_errors=5,\n",
    "        max_stream_messages_per_chunk=10,\n",
    "        max_stream_errors_per_chunk=2,\n",
    "        verbose_task_execution=verbose,\n",
    "        print_progress=print_progress,\n",
    "        log_file_path=log_file_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = execute_interaction_tests([5])\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert len(test_data.interaction) > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "\n",
    "NOW = datetime.now(ZoneInfo(\"Europe/Berlin\")).strftime(\"%Y-%m-%d %H:%M:%S %z\")\n",
    "RES = execute_interaction_tests(indices=None, verbose=False, log_file_path=f\"reports/test report {NOW}.json\")\n",
    "RES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "[(r.passed, r.crashed) for r in RES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Random other tests and snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "possible_answer = \"\"\"\n",
    "\n",
    "<execute>\n",
    "* list_decks()\n",
    "</execute>\"\"\"\n",
    "\n",
    "te._parse_llm_response(possible_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "substring_search_res = test_llmi.search_for_substring(\n",
    "    deck_id_str=\"*\", search_substring=\"change\", search_in_question=True, search_in_answer=True, case_sensitive=False\n",
    ")\n",
    "\n",
    "assert len(substring_search_res.cards) == 5\n",
    "\n",
    "for c in substring_search_res.cards:\n",
    "    print(c)\n",
    "    print(\"\\n===============================\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
